#+TITLE: lol

[[https://img.shields.io/github/v/tag/lepisma/lol.svg?style=flat-square]]

Lossless or Lossy. =lol= tells whether a given wav has gone through lossy encoding
in its lifetime.

=lol= needs examples of pure lossless audios of your use case (the use case
shouldn't matter much but this has to be validated) and a list of potential
audio transforms. Then it trains a model to predict whether a new set of audios
is lossless (1) or not (0).

#+begin_src shell
lol

Usage:
  lol train --audio-dir=<audio-dir> --transforms-file=<transforms-file> --output-model=<output-model>
  lol --audio-dir=<audio-dir> --model=<model> --output-csv=<output-csv>

Options:
  --transforms-file=<transforms-file>      Plain text file with lines mapping to ffmpeg lossy transforms.
#+end_src

Sample transforms file is present in ~./resources/~ directory.

** Modeling notes

Flipped mel filters give the following test results on a wav dataset:
#+begin_example
              precision    recall  f1-score   support

           0       1.00      1.00      1.00       112
           1       1.00      1.00      1.00       102

    accuracy                           1.00       214
   macro avg       1.00      1.00      1.00       214
weighted avg       1.00      1.00      1.00       214
#+end_example

As compared to regular filters:
#+begin_example
              precision    recall  f1-score   support

           0       0.96      0.91      0.94       112
           1       0.91      0.96      0.93       102

    accuracy                           0.93       214
   macro avg       0.93      0.94      0.93       214
weighted avg       0.94      0.93      0.93       214
#+end_example

Of course you might get more mileage by complicating the model, but this looks
like a neat feature trick for the problem.
